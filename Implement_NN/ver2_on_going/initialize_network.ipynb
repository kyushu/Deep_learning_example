{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_network function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network= list()\n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "\n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden +1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "\n",
    "    return network\n",
    "\n",
    "seed(1)\n",
    "network = initialize_network(2, 1, 2)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "akj: k = kth neuron of previous layer, \n",
    "     j = jth neuron of current layer  \n",
    "                    b00                    a00                   a10  \n",
    "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]  \n",
    "                     b00                    a00                                 b01                    a01  \n",
    "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "# weights : weight corresponding to neurons of previous layer\n",
    "# inputs  : output of neurons of previous layer \n",
    "def activate(weights, inputs, verbose=False):\n",
    "    \n",
    "    activation = weights[-1] # the last weight is bias\n",
    "    \n",
    "    # len(inputs) - 1 = we don't count bias in\n",
    "    for i in range(len(inputs) - 1):\n",
    "        if verbose:\n",
    "            print(\"activation:{}\".format(activation))\n",
    "            print(\"weight[{}]: {} * inputs[{}] = {}\".format(i, weights[i], i, inputs[i]))\n",
    "        activation += weights[i] * inputs[i]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"cur activation: {}\".format(activation))\n",
    "        \n",
    "    return activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transfer neruon activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward_propagate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row, verbose=False):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        if verbose:\n",
    "            print(\"layer: {}\".format(layer))\n",
    "        \n",
    "        for neuron in layer:\n",
    "            if verbose:\n",
    "                print(\"neuron: {}\".format(neuron))\n",
    "                \n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# row = input \n",
    "# the last element of row is bias\n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6213859615555266, 0.6573693455986976]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer_derivative function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "# y = 1/ (1 + exp(-z)))\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward_propagate_error function : Calculate Backpropagate error and store in neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "\n",
    "'''\n",
    "for each neuron of output layer\n",
    "    error = (expected - output) * transfer_derivative(output of output layer)\n",
    "\n",
    "for each neuron of hidden layer\n",
    "    error_kj = (weight_kj * error_j) * transfer_derivative(output_kj)\n",
    "    \n",
    "    假設目前在 \"h\" layer 的第 \"k\" 個 neuron\n",
    "    output_kj_h   = \"h\"   layer 的第 \"k\" 個 neuron 輸出到下一個 \"h+1\" layer 的第 \"j\" 個 neuron\n",
    "    weight_kj_h   = \"h\"   layer 的第 \"k\" 個 neuron 到下一個    \"h+1\" layer 的第 \"j\" 個 neuron 的 weigth\n",
    "    error_j_h+1   = \"h+1\" layer 的第 \"j\" 個 neuron 的 error\n",
    "                \n",
    "                以 output layer(第 L 個 layer) 前一個 hidden layer(L-1) 而言\n",
    "                \"L-1\" layer 的第 \"j\" 個 neuron 的 backpropagate error\n",
    "                error_j = (expected - output_Lj) * transfer_derivative(output_j_L)\n",
    "\n",
    "    error_k = sum(error_kj), j = 1 ~ n\n",
    "'''\n",
    "\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            #\n",
    "            # Hidden layer\n",
    "            #\n",
    "            # the error of neuron of hidden layer = weight * 後一個 layer 的 error\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    # neuron['delta] = back propagation error\n",
    "                    # 對於目前的 layer 的這個 neuron (假設為 k) 的 backpropagation error\n",
    "                    # 就是將 k 到下一個 layer 的每一個 neuron 的 weight 都乘上該 neuron(下一個 layer) 的 backpropagation error\n",
    "                    # 累加起來之後再乘上 transfer_derivative(k_output)\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            #\n",
    "            # The final layer = output layer\n",
    "            #\n",
    "            # the backpropagation error of output layer = (Real_Y - Predict_Y) * transfer_derivative(Predict_Y)\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        \n",
    "        # 將每個 neuron 的 error 存在 \"delta\"\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n",
      "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"
     ]
    }
   ],
   "source": [
    "expected = [0, 1]\n",
    "backward_propagate_error(network, expected)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update_weights function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "\n",
    "'''\n",
    "w = w + learning_rate * backpropagation_error * input\n",
    "'''\n",
    "\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row\n",
    "        if i != 0:\n",
    "            # 前一個 layer 的 output 等於目前 layer 的 input\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        \n",
    "        # update weight of all neuron of current layer\n",
    "        for neuron in network[i]:\n",
    "            # 目前的設定是 每個 neuron 的最後一個 wieght 是 bias\n",
    "            for j in range(len(inputs) - 1):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_network Fuction : Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        # a complete forward_propagate, backward_propagate process\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            # initialize an zero-value vector has same length with n_outputs\n",
    "            # expected 用來存每個 layer\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            # bias = 1\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i] - outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        \n",
    "        print(\">epoch=%d, lrate=%.3f, error=%.3f\" % (epoch, l_rate, sum_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example for training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_outputs: set([0, 1])\n",
      ">epoch=0, lrate=0.500, error=5.421\n",
      ">epoch=1, lrate=0.500, error=5.222\n",
      ">epoch=2, lrate=0.500, error=5.096\n",
      ">epoch=3, lrate=0.500, error=4.813\n",
      ">epoch=4, lrate=0.500, error=4.409\n",
      ">epoch=5, lrate=0.500, error=4.095\n",
      ">epoch=6, lrate=0.500, error=3.782\n",
      ">epoch=7, lrate=0.500, error=3.478\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.926\n",
      ">epoch=10, lrate=0.500, error=2.682\n",
      ">epoch=11, lrate=0.500, error=2.460\n",
      ">epoch=12, lrate=0.500, error=2.258\n",
      ">epoch=13, lrate=0.500, error=2.076\n",
      ">epoch=14, lrate=0.500, error=1.913\n",
      ">epoch=15, lrate=0.500, error=1.766\n",
      ">epoch=16, lrate=0.500, error=1.634\n",
      ">epoch=17, lrate=0.500, error=1.516\n",
      ">epoch=18, lrate=0.500, error=1.410\n",
      ">epoch=19, lrate=0.500, error=1.315\n",
      "[{'output': 0.025670375645295517, 'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799], 'delta': -0.00654781981492048}, {'output': 0.9693210809965882, 'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327], 'delta': 0.00012307882503115525}]\n",
      "[{'output': 0.2665068001469742, 'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829], 'delta': -0.052096995977393146}, {'output': 0.7272040729964842, 'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656], 'delta': 0.054116794759370944}]\n"
     ]
    }
   ],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "           [1.465489372,2.362125076,0],\n",
    "           [3.396561688,4.400293529,0],\n",
    "           [1.38807019,1.850220317,0],\n",
    "           [3.06407232,3.005305973,0],\n",
    "           [7.627531214,2.759262235,1],\n",
    "           [5.332441248,2.088626775,1],\n",
    "           [6.922596716,1.77106367,1],\n",
    "           [8.675418651,-0.242068655,1],\n",
    "           [7.673756466,3.508563011,1] \n",
    "          ]\n",
    "# dataset = [x1, x2, y], 所以 input 的個數為 2\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "# set() : 取得不重覆的 element\n",
    "# row[-1] = the last element of row = y, y 只有 (0, 1)\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "print(\"n_outputs: {}\".format(set([row[-1] for row in dataset])))\n",
    "      \n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "\n",
    "for layer in network:\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "for row in dataset:\n",
    "\tprediction = predict(network, row)\n",
    "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
